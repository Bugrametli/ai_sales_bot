{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640e4bbc",
   "metadata": {},
   "source": [
    "# Project: Clerk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b748fd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13727cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4bdf053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "111f6b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07900179",
   "metadata": {},
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ae19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38eb372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bb8ad",
   "metadata": {},
   "source": [
    "### Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62c3cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b298177",
   "metadata": {},
   "source": [
    "### Calculating Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "add191c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f92afa",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c72c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name):\n",
    "    import pinecone #library\n",
    "    from langchain.vectorstores import Pinecone #class\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings #text to vectors\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key = \"\" )\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52ef1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('Ok')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8c5bc",
   "metadata": {},
   "source": [
    " # Extra Code That I Have Written"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66b24f",
   "metadata": {},
   "source": [
    "## Santral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f98098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def customer_message(new_message_cust):\n",
    "  reply_customer = input(new_message_cust)\n",
    "  input = \"From Customer to Sales Representative\" + input\n",
    "  santral(input)\n",
    "\n",
    "\n",
    "\n",
    "def sales_rep_answers(new_message_sales)\n",
    "  #chat history'yi nerede tutacağız\n",
    "  result = ask_with_memory_sr(new_nessage_sales)\n",
    "  #chat history'yi nerede tutacağız\n",
    "  santral(result[0])\n",
    "\n",
    "def technical_partner_answers(new_message_tp)\n",
    "  #chat history'yi nerede tutacağız\n",
    "  result = ask_with_memory_sr(new_nessage_tp)\n",
    "  #chat history'yi nerede tutacağız\n",
    "  santral(result[0])\n",
    "\n",
    "def santral(input):\n",
    "  if \"From Main Guy to Technical Partner\" in input:\n",
    "    technical_partner_asnwers(\"There is one message from your sales representative\\n\"+input)\n",
    "  elif \"From Technical Partner to Main Guy\" in input:\n",
    "    sales_rep_answers(\"There is one message from your technical partner\\n\"+input)\n",
    "  elif \"From Main Guy to Customer\" in input:\n",
    "    customer_message(input.replace(\"From Main Guy to Customer\",\"\"))\n",
    "  elif \"From Customer to Main Guy\" in input:\n",
    "     sales_rep_answers(input)\n",
    "  else:\n",
    "    raise Exception(\"Sth went wrong, you gotta solve it now\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb32bc8",
   "metadata": {},
   "source": [
    "# tp ask with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory_tp(vector_store, question_tp, chat_history_tp=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain #chat history\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=1) #by default it uses gpt turbo 3.5\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever) #chain object\n",
    "    result = crc({'question': question_tp, 'chat_history': chat_history_tp})\n",
    "    chat_history.append((question_tp, result['answer']))\n",
    "\n",
    "    return result, chat_history\n",
    "    #Implement API\n",
    "    #There will be no vector store\n",
    "    #Retriever stays here for what"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb00f61",
   "metadata": {},
   "source": [
    "# sr ask with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory_sr(question_sr, chat_history_sr=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain #chat history\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=1, model_name = \"gpt-4\",  openai_api_key = \"\") #by default it uses gpt turbo 3.5\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm) #chain object\n",
    "    result = crc({'question': question_sr, 'chat_history': chat_history_sr})\n",
    "    chat_history.append((question_sr, result'answer']))\n",
    "\n",
    "    return result, chat_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a94cb",
   "metadata": {},
   "source": [
    "### Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08b0a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.run(q)\n",
    "    return answer\n",
    "    \n",
    "    \n",
    "def ask_with_memory(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain #chat history\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1) #by default it uses gpt turbo 3.5\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever) #chain object\n",
    "    result = crc({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efe8fc",
   "metadata": {},
   "source": [
    "### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23fd7147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/adnanbugrametli/Desktop/ChatGPT(Novus)/ChatGPT_Data.pdf\n",
      "You have 67 pages in your data\n",
      "There are 1074 characters in the page\n"
     ]
    }
   ],
   "source": [
    "data = load_document('/Users/adnanbugrametli/Desktop/ChatGPT(Novus)/ChatGPT_Data.pdf')\n",
    "# print(data[1].page_content)\n",
    "# print(data[10].metadata)\n",
    "\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64b5d7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "# print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faa75175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 56619\n",
      "Embedding Cost in USD: 0.022648\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8525a620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b158ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is about diamond certification and how experts can help you understand what it means for any created diamond when you book an appointment. It also mentions that having diamond certification ensures that you can focus on the proposal itself instead of worrying.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00ddb48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n",
      "Question #1: quit\n",
      "Quitting ... bye bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    \n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "775915c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d08d3",
   "metadata": {},
   "source": [
    "### Ask with Memory Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e81a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "i = 1\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"Write Quit or Exit to quit\")\n",
    "while True:\n",
    "    q = input(f\"Question #{i}\")\n",
    "    i = i + 1\n",
    "    if q.lower() in [\"quit\",\"exit\"]:\n",
    "        print(\"Qutting\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    result, _ = ask_with_memory(vector_store, q, chat_history)\n",
    "    print (result['answer'])\n",
    "    print(\"----------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
